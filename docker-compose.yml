# =============================================================================
# ConCall Local Model - Docker Compose
# 本地端 AI 即時會議紀錄與翻譯系統
# Hardware: 2× NVIDIA RTX 4090 (24GB VRAM each)
#
# Architecture (4 services):
#   redis      — Message Broker
#   app-core   — Web UI + Gateway + Translation + Summary (CPU Only)
#   worker-asr — ASR + VAD + Diarization (GPU 0)
#   worker-llm — Qwen2.5-32B LLM API (GPU 1)
# =============================================================================

services:

  # ---------------------------------------------------------------------------
  # Service A: Redis — 訊息代理與音訊緩衝
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: concall-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - concall-net

  # ---------------------------------------------------------------------------
  # Service B: app-core — Web UI + Gateway + Translation + Summary (CPU Only)
  # ---------------------------------------------------------------------------
  app-core:
    build:
      context: .
      dockerfile: core/Dockerfile
    container_name: concall-core
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./data:/app/data
    environment:
      - REDIS_URL=redis://redis:6379/0
      - LLM_BASE_URL=http://worker-llm:8000/v1
      - LLM_MODEL=Qwen/Qwen2.5-32B-Instruct-AWQ
      - VLLM_CONTAINER_NAME=concall-vllm
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - concall-net

  # ---------------------------------------------------------------------------
  # Service B2: worker-intelligence — Translation + Summary (CPU Only)
  # 訂閱 Redis transcriptions，呼叫 vLLM 進行翻譯和摘要
  # ---------------------------------------------------------------------------
  worker-intelligence:
    build:
      context: .
      dockerfile: worker-intelligence/Dockerfile
    container_name: concall-intelligence
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - REDIS_URL=redis://redis:6379/0
      - LLM_BASE_URL=http://worker-llm:8000/v1
      - LLM_MODEL=Qwen/Qwen2.5-32B-Instruct-AWQ
      - VLLM_CONTAINER_NAME=concall-vllm
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - concall-net

  # ---------------------------------------------------------------------------
  # Service C: worker-asr — ASR + VAD + Diarization (GPU 0)
  # GPU 0 混合工作區: Whisper (~3GB) + VAD (~0.1GB) + Pyannote (~2.5GB) ≈ 5.6GB
  # ---------------------------------------------------------------------------
  worker-asr:
    build:
      context: .
      dockerfile: workers/asr/Dockerfile
    container_name: concall-asr
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379/0
      - CUDA_VISIBLE_DEVICES=0
      - ASR_MODEL_SIZE=large-v3
      - ASR_COMPUTE_TYPE=float16
      - HF_TOKEN=${HF_TOKEN}
      - HF_HUB_OFFLINE=0
      - DIARIZATION_INTERVAL=30
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    volumes:
      - E:\work\AI_Antigravity\Test\aimodel:/root/.cache/huggingface/hub
      - torch_cache:/root/.cache/torch
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - concall-net

  # ---------------------------------------------------------------------------
  # Service D: worker-llm — LLM OpenAI-Compatible API (GPU 1 獨佔)
  # GPU 1: Qwen2.5-32B-AWQ (~18.5GB) + KV Cache (~5.5GB) = 24/24GB
  # ---------------------------------------------------------------------------
  worker-llm:
    image: vllm/vllm-openai:v0.6.6.post1
    container_name: concall-vllm
    restart: unless-stopped
    command:
      - "--model"
      - "Qwen/Qwen2.5-32B-Instruct-AWQ"
      - "--quantization"
      - "awq"
      - "--dtype"
      - "float16"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "4"
      - "--enforce-eager"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
    ports:
      - "8001:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - HF_TOKEN=${HF_TOKEN}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [ gpu ]
    volumes:
      - model_cache:/root/.cache/huggingface
    networks:
      - concall-net

volumes:
  redis_data:
    driver: local
  model_cache:
    driver: local
  torch_cache:
    driver: local

networks:
  concall-net:
    driver: bridge
