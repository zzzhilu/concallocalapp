"""
ConCall Local Model â€” worker-intelligence

ç¿»è­¯/æ‘˜è¦ç·¨æ’å™¨ (CPU Only)ï¼š
- è¨‚é–± ch:transcriptions â†’ å³æ™‚ç¿»è­¯ (ä¸­â†”è‹±è‡ªå‹•åµæ¸¬)
- ç›£è½ session çµæŸä¿¡è™Ÿ â†’ ç”Ÿæˆæœƒè­°æ‘˜è¦
- é€é OpenAI SDK å‘¼å« vLLM Server (http://vllm-server:8000/v1)

ä¸ç›´æ¥è¼‰å…¥ä»»ä½• GPU æ¨¡å‹ï¼Œæ‰€æœ‰æ¨è«–é€é HTTP API å®Œæˆã€‚
å…·å‚™ Docker æ§åˆ¶èƒ½åŠ›ï¼Œå¯æ ¹æ“šéœ€æ±‚å•Ÿå‹•/åœæ­¢ vLLM å®¹å™¨ä»¥ç¯€çœ GPU è³‡æºã€‚
"""

import asyncio
import json
import hashlib
import logging
import os
import re
import time
from typing import Optional

import redis.asyncio as aioredis
from openai import AsyncOpenAI
import docker

# å…±ç”¨æ¨¡çµ„
import sys
sys.path.insert(0, "/app")
from core.redis_keys import (
    SESSION_TRANSCRIPT_PREFIX,
    SESSION_LANG_PREFIX,
    CHANNEL_TRANSCRIPTIONS,
    CHANNEL_TRANSLATIONS,
    CHANNEL_SUMMARY,
    CHANNEL_STATUS,
    SESSION_END_SIGNAL,
    GLOSSARY_KEY,
)

# ---------------------------------------------------------------------------
# è¨­å®š
# ---------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
)
logger = logging.getLogger("worker-intelligence")

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://vllm-server:8000/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "Qwen/Qwen2.5-32B-Instruct-AWQ")

# ç¿»è­¯/æ‘˜è¦è¨­å®š
TRANSLATE_MAX_TOKENS = 512
SUMMARY_MAX_TOKENS = 2048
CHUNK_SUMMARY_MAX_TOKENS = 1024

# åˆ†æ®µæ‘˜è¦è¨­å®š
CHUNK_SIZE = 5000          # æ¯æ®µæœ€å¤§å­—å…ƒæ•¸ï¼ˆç´„ 25-35 åˆ†é˜æœƒè­°ï¼‰
CHUNK_THRESHOLD = 10000    # è¶…éæ­¤å­—å…ƒæ•¸å•Ÿå‹•åˆ†æ®µæ‘˜è¦

# é‡è©¦è¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 2

# ---------------------------------------------------------------------------
# Docker Control
# ---------------------------------------------------------------------------
docker_client = docker.from_env()
VLLM_CONTAINER_NAME = "concall-vllm"

def manage_vllm(action: str):
    """ç®¡ç† vLLM å®¹å™¨ç‹€æ…‹ (start/stop)"""
    try:
        container = docker_client.containers.get(VLLM_CONTAINER_NAME)
        if action == "start":
            if container.status != "running":
                logger.info(f"å•Ÿå‹• vLLM å®¹å™¨ ({VLLM_CONTAINER_NAME})...")
                container.start()
            else:
                logger.debug("vLLM å®¹å™¨å·²åœ¨é‹è¡Œã€‚")
        elif action == "stop":
            if container.status == "running":
                logger.info(f"åœæ­¢ vLLM å®¹å™¨ ({VLLM_CONTAINER_NAME}) ä»¥é‡‹æ”¾ GPU...")
                container.stop()
            else:
                logger.debug("vLLM å®¹å™¨å·²åœæ­¢ã€‚")
    except Exception as e:
        logger.error(f"Docker æ§åˆ¶å¤±æ•— ({action}): {e}")

# ---------------------------------------------------------------------------
# LLM Client
# ---------------------------------------------------------------------------
llm_client: Optional[AsyncOpenAI] = None

def init_llm_client() -> AsyncOpenAI:
    """åˆå§‹åŒ– OpenAI-Compatible LLM Clientã€‚"""
    return AsyncOpenAI(
        base_url=LLM_BASE_URL,
        api_key="not-needed",  # æœ¬åœ°éƒ¨ç½²ä¸éœ€è¦ API key
        timeout=120.0,
        max_retries=MAX_RETRIES,
    )

async def ensure_llm_ready(timeout=120):
    """ç¢ºä¿ vLLM å·²å°±ç·’ (è‹¥å®¹å™¨æœªå•Ÿå‹•å‰‡å•Ÿå‹•å®ƒ)ã€‚"""
    global llm_client
    
    # 1. æª¢æŸ¥ä¸¦å•Ÿå‹•å®¹å™¨
    manage_vllm("start")
    
    # 2. åˆå§‹åŒ– Client
    if not llm_client:
        llm_client = init_llm_client()
    
    # 3. ç­‰å¾… API å°±ç·’
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            await llm_client.models.list()
            return True
        except Exception:
            await asyncio.sleep(2)
            
    logger.error("âŒ vLLM Server å•Ÿå‹•è¶…æ™‚ã€‚")
    return False

# ---------------------------------------------------------------------------
# ç¿»è­¯åŠŸèƒ½
# ---------------------------------------------------------------------------
TRANSLATE_PROMPT_EN2ZH = """ä½ æ˜¯å³æ™‚å£è­¯å“¡ã€‚ç›´æ¥è¼¸å‡ºç¹é«”ä¸­æ–‡ç¿»è­¯ï¼Œä¸è¦ä»»ä½•èªªæ˜ã€è§£é‡‹æˆ–æ€è€ƒéç¨‹ã€‚å¿½ç•¥å£èªè´…å­—ï¼Œä¿ç•™å°ˆæœ‰åè©åŸæ–‡ã€‚"""

TRANSLATE_PROMPT_ZH2EN = """You are a real-time translator. Output ONLY the English translation. No explanations, no thinking, no extra text."""

def strip_think_tags(text: str) -> str:
    """ç§»é™¤ LLM å›æ‡‰ä¸­çš„ <think>...</think> æ¨™ç±¤åŠå…¶å…§å®¹ã€‚"""
    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
    return cleaned if cleaned else text


# ---------------------------------------------------------------------------
# æ¼¸é€²å¼ç¿»è­¯ç‹€æ…‹
# ---------------------------------------------------------------------------
# æ¯å€‹ session è¿½è¹¤æœ€è¿‘çš„ segmentsï¼Œåˆä½µç¿»è­¯ä»¥ç”¢ç”Ÿæ›´å¥½çš„çµæœ
_session_segments: dict[str, list[dict]] = {}  # session_id -> [{text, seg_id, timestamp}]
_last_revision_hash: dict[str, str] = {}       # session_id -> md5 hash (å»é‡)
SEGMENT_MERGE_WINDOW = 5    # æœ€å¤šåˆä½µæœ€è¿‘ N å€‹ segments
REVISION_MIN_CHARS = 30     # åˆä½µæ–‡å­—è¶…éæ­¤é•·åº¦æ‰è§¸ç™¼ä¿®æ­£ç¿»è­¯
REVISION_MAX_CHARS = 200    # åˆä½µæ–‡å­—è¶…éæ­¤é•·åº¦ä¸å†åˆä½µï¼ˆé¿å…éé•·å¥å­ï¼‰
SENTENCE_END_RE = re.compile(r'[.!?ã€‚ï¼ï¼Ÿï¼›ï¼š\n]\s*$')  # å¥å°¾æ¨™é»åµæ¸¬


# ---------------------------------------------------------------------------
# è©å½™è¡¨å¿«å–ï¼ˆé¿å…æ¯æ¬¡ç¿»è­¯éƒ½é–‹æ–° Redis é€£ç·šï¼‰
# ---------------------------------------------------------------------------
_glossary_cache: list | None = None
_glossary_cache_ts: float = 0
GLOSSARY_CACHE_TTL = 30  # å¿«å– 30 ç§’


async def get_glossary_terms(redis_conn: aioredis.Redis) -> list:
    """å¾ Redis è®€å–è©å½™è¡¨ï¼Œå¸¶ TTL å¿«å–ã€‚"""
    global _glossary_cache, _glossary_cache_ts
    now = time.time()
    if _glossary_cache is not None and (now - _glossary_cache_ts) < GLOSSARY_CACHE_TTL:
        return _glossary_cache
    try:
        glossary_json = await redis_conn.get(GLOSSARY_KEY)
        if glossary_json:
            _glossary_cache = json.loads(glossary_json)
        else:
            _glossary_cache = []
        _glossary_cache_ts = now
    except Exception as e:
        logger.warning(f"Failed to load glossary from Redis: {e}")
        if _glossary_cache is None:
            _glossary_cache = []
    return _glossary_cache


def _build_glossary_suffix(terms: list, target_lang: str = "zh") -> str:
    """æ ¹æ“šè©å½™è¡¨å»ºæ§‹ prompt å¾Œç¶´ã€‚"""
    if not terms:
        return ""
    if target_lang == "zh":
        glossary_lines = "\n".join(f"- {t['en']} â†’ {t['zh']}" for t in terms if t.get('en') and t.get('zh'))
    else:
        glossary_lines = "\n".join(f"- {t['zh']} â†’ {t['en']}" for t in terms if t.get('en') and t.get('zh'))
    if not glossary_lines:
        return ""
    return f"\nå°ˆæœ‰åè©å°ç…§ï¼š\n{glossary_lines}"


async def translate_text(text: str, source_lang: str = "auto", redis_conn: aioredis.Redis = None) -> dict:
    """ç¿»è­¯æ–‡å­—ã€‚"""
    global llm_client
    
    # è‹¥ LLM æœªå°±ç·’ (ä¾‹å¦‚ä¸­æ–‡æ¨¡å¼ä¸‹ GPU é—œé–‰)ï¼Œç›´æ¥å›å‚³åŸæ–‡ä¸¦æ¨™è¨˜æœªç¿»è­¯
    if not llm_client:
        # å˜—è©¦åˆå§‹åŒ–ä¸€æ¬¡ï¼Œå¦‚æœå®¹å™¨æ˜¯é–‹çš„å°±èƒ½é€£ä¸Š
        try:
           manage_vllm("start") # ç¢ºä¿å®¹å™¨æ˜¯é–‹çš„ (å¦‚æœæ˜¯ç¿»è­¯æ¨¡å¼)
           llm_client = init_llm_client()
        except:
           pass

    if not text.strip():
        return {"translated_text": "", "source_lang": source_lang, "target_lang": "unknown"}

    try:
        if not llm_client:
             return {"translated_text": "(ç¿»è­¯æœªå•Ÿç”¨)", "source_lang": source_lang, "target_lang": "unknown", "error": "LLM offline"}

        # åµæ¸¬èªè¨€æ–¹å‘
        if source_lang == "auto":
            # ç°¡æ˜“åµæ¸¬: åŒ…å« CJK å­—ç¬¦ â†’ ä¸­æ–‡
            has_cjk = any('\u4e00' <= c <= '\u9fff' for c in text)
            source_lang = "zh" if has_cjk else "en"

        target_lang = "en" if source_lang == "zh" else "zh"

        # æ ¹æ“šç¿»è­¯æ–¹å‘é¸æ“‡å°æ‡‰çš„ system prompt
        system_prompt = TRANSLATE_PROMPT_EN2ZH if target_lang == "zh" else TRANSLATE_PROMPT_ZH2EN

        # æ³¨å…¥è‡ªè¨‚è©å½™è¡¨ï¼ˆä½¿ç”¨å¿«å–ï¼‰
        if redis_conn:
            terms = await get_glossary_terms(redis_conn)
            system_prompt += _build_glossary_suffix(terms, target_lang)

        response = await llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text},
            ],
            max_tokens=TRANSLATE_MAX_TOKENS,
            temperature=0.3,
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
        )

        translated = response.choices[0].message.content.strip()
        # é˜²ç¦¦æ€§éæ¿¾ï¼šç§»é™¤ä»»ä½• <think> æ¨™ç±¤
        translated = strip_think_tags(translated)

        return {
            "translated_text": translated,
            "source_lang": source_lang,
            "target_lang": target_lang,
        }

    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—: {e}")
        return {"translated_text": text, "error": str(e)}

# ---------------------------------------------------------------------------
# æ‘˜è¦åŠŸèƒ½
# ---------------------------------------------------------------------------
SUMMARY_SYSTEM_PROMPT = """ä½ æ˜¯å°ˆæ¥­çš„æœƒè­°ç´€éŒ„æ•´ç†å°ˆå®¶ã€‚æ ¹æ“šæœƒè­°é€å­—ç¨¿ï¼Œç”¨ç¹é«”ä¸­æ–‡æŒ‰ä»¥ä¸‹ Markdown æ ¼å¼ç”¢å‡ºçµæ§‹åŒ–æœƒè­°ç´€éŒ„ã€‚ç›´æ¥å¡«å¯«ï¼Œä¸è¦åŠ é¡å¤–èªªæ˜ã€‚è‹¥è³‡è¨Šä¸è¶³å¯çœç•¥è©²å€å¡Šã€‚

# [æœƒè­°æ¨™é¡Œ]

**æ—¥æœŸ**ï¼š[å¾å°è©±æ¨æ–·æˆ–æ¨™è¨˜ä»Šæ—¥æ—¥æœŸ]
**åƒèˆ‡è€…**ï¼š[å¾èªªè©±è€…æ¨™ç±¤åˆ—å‡ºï¼Œè‹¥ç„¡æ¨™ç±¤å¯«ã€Œæœªæ¨™è¨»ã€]

## é‡é»è¨è«–

### [è­°é¡Œ 1]
- è¨è«–æ‘˜è¦ï¼ˆ1-2å¥ï¼‰
- é—œéµè§€é»

### [è­°é¡Œ 2]
- ...ï¼ˆä¾å¯¦éš›è­°é¡Œæ•¸é‡å±•é–‹ï¼‰

## æ±ºè­°äº‹é …
- âœ… [æ±ºè­° 1]
- âœ… [æ±ºè­° 2]

## å¾…è¾¦äº‹é …

| äº‹é … | è² è²¬äºº | æœŸé™ | ç‹€æ…‹ |
|------|--------|------|------|
| ä»»å‹™æè¿° | äººå | æ—¥æœŸ | [ ] å¾…è¾¦ |

## å¾ŒçºŒæ­¥é©Ÿ
- [ä¸‹ä¸€æ­¥è¡Œå‹•]

## å¾…è­°äº‹é …
- [å»¶å¾Œè¨è«–çš„é …ç›®]

è¦å‰‡ï¼š
1. å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡
2. ç°¡æ½”æ‰¼è¦ï¼Œæ¯é …é™1-2å¥
3. é‡é»åœ¨çµæœå’Œè¡Œå‹•ï¼Œééç¨‹
4. è‹¥æœ‰èªªè©±è€…æ¨™ç±¤è«‹ä¿ç•™
5. æ±ºè­°å’Œå¾…è¾¦å¿…é ˆå…·é«”ã€å¯è¿½è¹¤"""

# ---------------------------------------------------------------------------
# åˆ†æ®µæ‘˜è¦ Prompts
# ---------------------------------------------------------------------------
CHUNK_SUMMARY_PROMPT = """ä½ æ˜¯æœƒè­°ç´€éŒ„æ•´ç†å°ˆå®¶ã€‚ä»¥ä¸‹æ˜¯ä¸€æ®µæœƒè­°é€å­—ç¨¿ç‰‡æ®µï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡æå–é‡é»ï¼š

1. åˆ—å‡ºæ‰€æœ‰è¨è«–çš„è­°é¡Œå’Œé—œéµè§€é»
2. åˆ—å‡ºä»»ä½•æ±ºè­°æˆ–å¾…è¾¦äº‹é …
3. ä¿ç•™èªªè©±è€…æ¨™ç±¤ï¼ˆå¦‚æœ‰ï¼‰
4. ç°¡æ½”æ‰¼è¦ï¼Œåªä¿ç•™é‡è¦è³‡è¨Š

ç›´æ¥è¼¸å‡ºæ‘˜è¦ï¼Œä¸éœ€é¡å¤–èªªæ˜ã€‚"""

MERGE_SUMMARY_PROMPT = """ä½ æ˜¯å°ˆæ¥­çš„æœƒè­°ç´€éŒ„æ•´ç†å°ˆå®¶ã€‚ä»¥ä¸‹æ˜¯åŒä¸€å ´æœƒè­°ä¸åŒæ™‚æ®µçš„åˆ†æ®µæ‘˜è¦ã€‚
è«‹å°‡å®ƒå€‘æ•´åˆç‚ºä¸€ä»½å®Œæ•´çš„çµæ§‹åŒ–æœƒè­°ç´€éŒ„ï¼Œç”¨ç¹é«”ä¸­æ–‡æŒ‰ä»¥ä¸‹ Markdown æ ¼å¼è¼¸å‡ºï¼š

# [æœƒè­°æ¨™é¡Œ]

**æ—¥æœŸ**ï¼š[å¾å°è©±æ¨æ–·æˆ–æ¨™è¨˜ä»Šæ—¥æ—¥æœŸ]
**åƒèˆ‡è€…**ï¼š[å¾èªªè©±è€…æ¨™ç±¤åˆ—å‡ºï¼Œè‹¥ç„¡æ¨™ç±¤å¯«ã€Œæœªæ¨™è¨»ã€]

## é‡é»è¨è«–

### [è­°é¡Œ 1]
- è¨è«–æ‘˜è¦ï¼ˆ1-2å¥ï¼‰
- é—œéµè§€é»

## æ±ºè­°äº‹é …
- âœ… [æ±ºè­° 1]

## å¾…è¾¦äº‹é …

| äº‹é … | è² è²¬äºº | æœŸé™ | ç‹€æ…‹ |
|------|--------|------|------|
| ä»»å‹™æè¿° | äººå | æ—¥æœŸ | [ ] å¾…è¾¦ |

## å¾ŒçºŒæ­¥é©Ÿ
- [ä¸‹ä¸€æ­¥è¡Œå‹•]

è¦å‰‡ï¼š
1. åˆä½µç›¸åŒè­°é¡Œï¼Œå»é™¤é‡è¤‡å…§å®¹
2. å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡
3. ç°¡æ½”æ‰¼è¦
4. æ±ºè­°å’Œå¾…è¾¦å¿…é ˆå…·é«”ã€å¯è¿½è¹¤"""


async def summarize_chunk(chunk_text: str, chunk_index: int, total_chunks: int, glossary_suffix: str = "") -> str:
    """å°å–®æ®µé€å­—ç¨¿ç”Ÿæˆç²¾ç°¡æ‘˜è¦ã€‚"""
    try:
        system_prompt = CHUNK_SUMMARY_PROMPT + glossary_suffix
        response = await llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ä»¥ä¸‹æ˜¯æœƒè­°ç¬¬ {chunk_index}/{total_chunks} æ®µé€å­—ç¨¿ï¼š\n\n{chunk_text}"},
            ],
            max_tokens=CHUNK_SUMMARY_MAX_TOKENS,
            temperature=0.3,
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
        )
        result = response.choices[0].message.content.strip()
        return strip_think_tags(result)
    except Exception as e:
        logger.error(f"æ®µè½ {chunk_index} æ‘˜è¦å¤±æ•—: {e}")
        return f"ï¼ˆç¬¬ {chunk_index} æ®µæ‘˜è¦å¤±æ•—ï¼‰"


def split_transcript_into_chunks(text: str, chunk_size: int = CHUNK_SIZE) -> list[str]:
    """å°‡é€å­—ç¨¿æŒ‰è¡Œåˆ‡åˆ†ç‚ºå¤šå€‹ä¸è¶…é chunk_size å­—å…ƒçš„æ®µè½ã€‚"""
    lines = text.split("\n")
    chunks = []
    current_chunk = []
    current_len = 0

    for line in lines:
        line_len = len(line) + 1  # +1 for newline
        if current_len + line_len > chunk_size and current_chunk:
            chunks.append("\n".join(current_chunk))
            current_chunk = [line]
            current_len = line_len
        else:
            current_chunk.append(line)
            current_len += line_len

    if current_chunk:
        chunks.append("\n".join(current_chunk))

    return chunks


async def generate_summary(session_id: str, redis_conn: aioredis.Redis) -> str:
    """ç”Ÿæˆæœƒè­°æ‘˜è¦ï¼ˆä¸²æµæ¨¡å¼ï¼‰ã€‚è¶…é CHUNK_THRESHOLD å­—å…ƒè‡ªå‹•å•Ÿå‹•åˆ†æ®µæ‘˜è¦ã€‚"""
    
    # 1. ç¢ºä¿ vLLM å·²å•Ÿå‹•
    logger.info(f"Session {session_id}: æº–å‚™ç”Ÿæˆæ‘˜è¦ï¼Œæ­£åœ¨å–šé†’ GPU...")
    ready = await ensure_llm_ready()
    if not ready:
        return "âŒ GPU å–šé†’å¤±æ•—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

    # 2. å¾ Redis å–å‡ºæ‰€æœ‰è½‰å¯«ç´€éŒ„
    transcript_key = SESSION_TRANSCRIPT_PREFIX + session_id
    records = await redis_conn.lrange(transcript_key, 0, -1)

    if not records:
        manage_vllm("stop")
        return "âš ï¸ æ­¤æœƒè­°æ²’æœ‰è½‰å¯«ç´€éŒ„ã€‚"

    # 3. çµ„åˆå®Œæ•´çš„è½‰å¯«æ–‡æœ¬
    full_transcript_parts = []
    for record_str in records:
        try:
            record = json.loads(record_str)
            text = record.get("text", "")
            timestamp = record.get("timestamp", 0)
            if timestamp:
                from datetime import datetime
                time_str = datetime.fromtimestamp(timestamp).strftime("%H:%M:%S")
                full_transcript_parts.append(f"[{time_str}] {text}")
            else:
                full_transcript_parts.append(text)
        except json.JSONDecodeError:
            continue

    full_transcript = "\n".join(full_transcript_parts)

    if not full_transcript.strip():
        manage_vllm("stop")
        return "âš ï¸ è½‰å¯«ç´€éŒ„ç‚ºç©ºã€‚"

    transcript_len = len(full_transcript)
    logger.info(f"Session {session_id}: ç”Ÿæˆæ‘˜è¦ (è½‰å¯«é•·åº¦: {transcript_len} chars)...")

    # 4. è®€å–è©å½™è¡¨ä¸¦å»ºæ§‹å¾Œç¶´ï¼ˆæ‘˜è¦ä¹Ÿæ³¨å…¥å°ˆæœ‰åè©ï¼‰
    terms = await get_glossary_terms(redis_conn)
    glossary_suffix = _build_glossary_suffix(terms, "zh")

    # 5. åˆ¤æ–·æ˜¯å¦éœ€è¦åˆ†æ®µæ‘˜è¦
    use_chunked = transcript_len > CHUNK_THRESHOLD

    if use_chunked:
        # === MapReduce åˆ†æ®µæ‘˜è¦ ===
        chunks = split_transcript_into_chunks(full_transcript)
        total_chunks = len(chunks)
        logger.info(f"Session {session_id}: å•Ÿå‹•åˆ†æ®µæ‘˜è¦ â€” {total_chunks} æ®µ")

        # é€šçŸ¥å‰ç«¯é€²å…¥åˆ†æ®µæ¨¡å¼
        await redis_conn.publish(
            CHANNEL_SUMMARY,
            json.dumps({
                "session_id": session_id,
                "type": "summary_chunk",
                "chunk": f"ğŸ“‹ é€å­—ç¨¿è¼ƒé•·ï¼ˆ{transcript_len} å­—ï¼‰ï¼Œå•Ÿå‹•åˆ†æ®µæ‘˜è¦ï¼ˆ{total_chunks} æ®µï¼‰...\n\n",
                "timestamp": time.time(),
            }, ensure_ascii=False),
        )

        # Map: é€æ®µæ‘˜è¦
        chunk_summaries = []
        for i, chunk in enumerate(chunks, 1):
            await redis_conn.publish(
                CHANNEL_SUMMARY,
                json.dumps({
                    "session_id": session_id,
                    "type": "summary_chunk",
                    "chunk": f"â³ æ­£åœ¨è™•ç†ç¬¬ {i}/{total_chunks} æ®µ...\n",
                    "timestamp": time.time(),
                }, ensure_ascii=False),
            )
            summary = await summarize_chunk(chunk, i, total_chunks, glossary_suffix)
            chunk_summaries.append(f"### ç¬¬ {i} æ®µæ‘˜è¦\n{summary}")
            logger.info(f"Session {session_id}: æ®µ {i}/{total_chunks} æ‘˜è¦å®Œæˆ ({len(summary)} chars)")

        # Reduce: åˆä½µæ‰€æœ‰æ®µè½æ‘˜è¦
        merged_input = "\n\n".join(chunk_summaries)
        logger.info(f"Session {session_id}: åˆä½µ {total_chunks} æ®µæ‘˜è¦ ({len(merged_input)} chars)...")

        await redis_conn.publish(
            CHANNEL_SUMMARY,
            json.dumps({
                "session_id": session_id,
                "type": "summary_chunk",
                "chunk": f"\nğŸ”„ æ­£åœ¨æ•´åˆæ‰€æœ‰æ®µè½æ‘˜è¦...\n\n",
                "timestamp": time.time(),
            }, ensure_ascii=False),
        )

        # ç”¨ MERGE prompt ç”Ÿæˆæœ€çµ‚æ‘˜è¦ï¼ˆä¸²æµï¼‰
        summary_input = merged_input
        summary_system_prompt = MERGE_SUMMARY_PROMPT + glossary_suffix
    else:
        # === çŸ­æ–‡ç›´æ¥æ‘˜è¦ ===
        summary_input = full_transcript
        summary_system_prompt = SUMMARY_SYSTEM_PROMPT + glossary_suffix

    try:
        # ä¸²æµæ¨¡å¼ç”Ÿæˆæ‘˜è¦
        stream = await llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": summary_system_prompt},
                {"role": "user", "content": f"ä»¥ä¸‹æ˜¯æœƒè­°è½‰å¯«ç´€éŒ„ï¼š\n\n{summary_input}"},
            ],
            max_tokens=SUMMARY_MAX_TOKENS,
            temperature=0.3,
            stream=True,
            extra_body={"chat_template_kwargs": {"enable_thinking": False}},
        )

        full_summary = ""
        chunk_buffer = ""
        
        async for chunk in stream:
            delta = chunk.choices[0].delta
            if delta.content:
                chunk_buffer += delta.content
                full_summary += delta.content
                
                # æ¯æ”¶åˆ°ä¸€æ®µæœ‰æ„ç¾©çš„å…§å®¹å°±æ¨é€ï¼ˆé‡åˆ°æ›è¡Œæˆ–ç´¯ç© >= 20 å­—å…ƒï¼‰
                if '\n' in chunk_buffer or len(chunk_buffer) >= 20:
                    await redis_conn.publish(
                        CHANNEL_SUMMARY,
                        json.dumps({
                            "session_id": session_id,
                            "type": "summary_chunk",
                            "chunk": chunk_buffer,
                            "timestamp": time.time(),
                        }, ensure_ascii=False),
                    )
                    chunk_buffer = ""

        # ç™¼é€å‰©é¤˜çš„ buffer
        if chunk_buffer:
            await redis_conn.publish(
                CHANNEL_SUMMARY,
                json.dumps({
                    "session_id": session_id,
                    "type": "summary_chunk",
                    "chunk": chunk_buffer,
                    "timestamp": time.time(),
                }, ensure_ascii=False),
            )

        # ç™¼é€å®Œæˆä¿¡è™Ÿ
        await redis_conn.publish(
            CHANNEL_SUMMARY,
            json.dumps({
                "session_id": session_id,
                "type": "summary_done",
                "summary": full_summary,
                "timestamp": time.time(),
            }, ensure_ascii=False),
        )

        logger.info(f"Session {session_id}: æ‘˜è¦ç”Ÿæˆå®Œæˆ ({len(full_summary)} chars)")
        manage_vllm("stop")
        return full_summary

    except Exception as e:
        logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}", exc_info=True)
        manage_vllm("stop")
        return f"âŒ æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}"


# ---------------------------------------------------------------------------
# ä¸»è¿´åœˆ
# ---------------------------------------------------------------------------
async def translation_loop(redis_conn: aioredis.Redis):
    """å³æ™‚ç¿»è­¯è¿´åœˆï¼šè¨‚é–± ch:transcriptionsï¼Œç¿»è­¯å¾Œç™¼å¸ƒåˆ° ch:translationsã€‚
    
    æ”¯æ´æ¼¸é€²å¼ç¿»è­¯ä¿®æ­£ï¼šè¿½è¹¤æœ€è¿‘çš„ segmentsï¼Œç•¶å¥å­æ›´å®Œæ•´æ™‚è‡ªå‹•é‡æ–°ç¿»è­¯ã€‚
    """
    pubsub = redis_conn.pubsub()
    await pubsub.subscribe(CHANNEL_TRANSCRIPTIONS)
    logger.info("ç¿»è­¯è¿´åœˆå•Ÿå‹•ï¼Œè¨‚é–± ch:transcriptions...")

    try:
        async for message in pubsub.listen():
            if message["type"] != "message":
                continue

            try:
                data = json.loads(message["data"])
            except (json.JSONDecodeError, TypeError):
                continue

            text = data.get("text", "")
            session_id = data.get("session_id", "unknown")
            source_lang = data.get("language", "auto")
            
            # æª¢æŸ¥ session èªè¨€åå¥½ï¼šä¸­æ–‡æ¨¡å¼ç›´æ¥è·³éç¿»è­¯
            session_lang = await redis_conn.get(SESSION_LANG_PREFIX + session_id)
            if session_lang and session_lang == "zh":
                continue  # ä¸­æ–‡æœƒè­°æ¨¡å¼ï¼Œä¸éœ€è¦ç¿»è­¯

            if not text.strip():
                continue

            # --- æ¼¸é€²å¼ç¿»è­¯ï¼šè¿½è¹¤ segments ---
            seg_id = f"{session_id}_{int(time.time() * 1000)}"
            if session_id not in _session_segments:
                _session_segments[session_id] = []
            
            _session_segments[session_id].append({
                "text": text,
                "seg_id": seg_id,
                "timestamp": time.time(),
            })
            
            # ä¿æŒçª—å£å¤§å°
            if len(_session_segments[session_id]) > SEGMENT_MERGE_WINDOW:
                _session_segments[session_id] = _session_segments[session_id][-SEGMENT_MERGE_WINDOW:]

            # 1. å…ˆå³æ™‚ç¿»è­¯ç•¶å‰ segmentï¼ˆå¿«é€Ÿå›æ‡‰ï¼‰
            result = await translate_text(text, source_lang, redis_conn=redis_conn)
            
            if "error" in result:
                continue

            # ç™¼å¸ƒå³æ™‚ç¿»è­¯çµæœ
            translation_data = {
                "session_id": session_id,
                "original_text": text,
                "translated_text": result.get("translated_text", ""),
                "source_lang": result.get("source_lang", source_lang),
                "target_lang": result.get("target_lang", ""),
                "timestamp": time.time(),
                "seg_id": seg_id,
                "is_revision": False,
            }

            await redis_conn.publish(
                CHANNEL_TRANSLATIONS,
                json.dumps(translation_data, ensure_ascii=False),
            )

            logger.info(
                f"Session {session_id}: "
                f"[{result.get('source_lang','?')}â†’{result.get('target_lang','?')}] "
                f"{text[:40]}... â†’ {result.get('translated_text','')[:40]}..."
            )

            # 2. æ¼¸é€²å¼ä¿®æ­£ï¼šåµæ¸¬å¥å°¾æ‰è§¸ç™¼åˆä½µç¿»è­¯
            recent = _session_segments[session_id]
            has_sentence_end = bool(SENTENCE_END_RE.search(text.strip()))
            merged_text = " ".join(s["text"] for s in recent)
            merged_len = len(merged_text)

            should_revise = (
                len(recent) >= 2
                and merged_len >= REVISION_MIN_CHARS
                and merged_len <= REVISION_MAX_CHARS
                and has_sentence_end  # åªåœ¨å¥å°¾æ‰è§¸ç™¼ä¿®æ­£
            )

            if should_revise:
                # å»é‡ï¼šæª¢æŸ¥æ˜¯å¦å’Œä¸Šæ¬¡åˆä½µçš„å…§å®¹ç›¸åŒ
                text_hash = hashlib.md5(merged_text.encode()).hexdigest()
                if text_hash != _last_revision_hash.get(session_id):
                    _last_revision_hash[session_id] = text_hash
                    revision_result = await translate_text(merged_text, source_lang, redis_conn=redis_conn)
                    if "error" not in revision_result:
                        revision_data = {
                            "session_id": session_id,
                            "original_text": merged_text,
                            "translated_text": revision_result.get("translated_text", ""),
                            "source_lang": revision_result.get("source_lang", source_lang),
                            "target_lang": revision_result.get("target_lang", ""),
                            "timestamp": time.time(),
                            "seg_ids": [s["seg_id"] for s in recent],
                            "is_revision": True,
                        }
                        await redis_conn.publish(
                            CHANNEL_TRANSLATIONS,
                            json.dumps(revision_data, ensure_ascii=False),
                        )
                        logger.info(
                            f"Session {session_id}: [ä¿®æ­£ç¿»è­¯] "
                            f"åˆä½µ {len(recent)} æ®µ â†’ {revision_result.get('translated_text','')[:60]}..."
                        )

                # å¥å­å®Œæˆ â†’ æ¸…ç©º pendingï¼Œé–‹å§‹æ–°å¥å­
                _session_segments[session_id] = []
            elif merged_len > REVISION_MAX_CHARS:
                # è¶…é•·ä½†æœªæ–·å¥ â†’ å¼·åˆ¶æ¸…ç©ºé¿å…ç„¡é™å †ç©
                _session_segments[session_id] = recent[-1:]

    except asyncio.CancelledError:
        logger.info("ç¿»è­¯è¿´åœˆå–æ¶ˆã€‚")
    finally:
        await pubsub.unsubscribe(CHANNEL_TRANSCRIPTIONS)


async def summary_monitor(redis_conn: aioredis.Redis):
    """ç›£æ§ session çµæŸä¿¡è™Ÿï¼Œè§¸ç™¼æ‘˜è¦ç”Ÿæˆã€‚"""
    pubsub = redis_conn.pubsub()
    await pubsub.subscribe(CHANNEL_STATUS)
    logger.info("æ‘˜è¦ç›£æ§å•Ÿå‹•ï¼Œç›£è¯ session çµæŸä¿¡è™Ÿ...")

    try:
        async for message in pubsub.listen():
            if message["type"] != "message":
                continue

            try:
                data = json.loads(message["data"])
            except (json.JSONDecodeError, TypeError):
                continue

            status = data.get("status", "")
            session_id = data.get("session_id", "")

            if status in ("session_ended",) and session_id:
                logger.info(f"Session {session_id}: æœƒè­°çµæŸï¼Œé–‹å§‹ç”Ÿæˆæ‘˜è¦...")

                # ç­‰å¾…ç‰‡åˆ»ï¼Œç¢ºä¿æœ€å¾Œçš„è½‰å¯«çµæœå·²è™•ç†å®Œ
                await asyncio.sleep(3)

                # ç”Ÿæˆæ‘˜è¦ï¼ˆå…§éƒ¨å·²åšä¸²æµç™¼å¸ƒï¼‰
                summary = await generate_summary(session_id, redis_conn)

                # å¦‚æœæ˜¯éŒ¯èª¤è¨Šæ¯ï¼ˆéä¸²æµæˆåŠŸï¼‰ï¼Œç™¼ä½ˆä¸€æ¬¡æ€§çµæœ
                if summary.startswith("âŒ") or summary.startswith("âš ï¸"):
                    await redis_conn.publish(
                        CHANNEL_SUMMARY,
                        json.dumps({
                            "session_id": session_id,
                            "type": "summary_done",
                            "summary": summary,
                            "timestamp": time.time(),
                        }, ensure_ascii=False),
                    )

                logger.info(f"Session {session_id}: æ‘˜è¦æµç¨‹çµæŸã€‚")

    except asyncio.CancelledError:
        logger.info("æ‘˜è¦ç›£æ§å–æ¶ˆã€‚")
    finally:
        await pubsub.unsubscribe(CHANNEL_STATUS)


# ---------------------------------------------------------------------------
# é€²å…¥é»
# ---------------------------------------------------------------------------
async def main():
    """ä¸»å…¥å£ã€‚"""
    global llm_client

    logger.info("=" * 60)
    logger.info("ConCall worker-intelligence å•Ÿå‹•ä¸­...")
    logger.info("  å…·å‚™ Docker æ§åˆ¶èƒ½åŠ›ï¼šæ”¯æ´è‡ªå‹•é‡‹æ”¾ GPU")
    logger.info("=" * 60)

    # åˆå§‹åŒ– Redis é€£ç·š
    redis_conn = aioredis.from_url(REDIS_URL, decode_responses=True)

    # å•Ÿå‹•è¿´åœˆ
    try:
        await asyncio.gather(
            translation_loop(redis_conn),
            summary_monitor(redis_conn),
        )
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿã€‚")
    finally:
        await redis_conn.aclose()
        logger.info("worker-intelligence å·²é—œé–‰ã€‚")


if __name__ == "__main__":
    asyncio.run(main())
